{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604a2ec7-9ca9-43a7-84a9-fb38de52cf22",
   "metadata": {},
   "source": [
    "# Training Notebook\n",
    "\n",
    "Date: **26-02-2022**\n",
    "\n",
    "In 'preprocess', we have prepared our data; now we will train.\n",
    "\n",
    "[Template Notebook](https://colab.research.google.com/github/lcroffi/CNN/blob/master/CNN_InceptionResNetV2.ipynb#scrollTo=qC04FoyQn_08) and ideas from [2017 kaggle competition winners](https://arxiv.org/pdf/1806.06193.pdf)\n",
    "\n",
    "## Approach:\n",
    "-  Start with `Animalia` to test; test results, and with augmented also\n",
    "-  Train with entirety of data; see what accuracy we get\n",
    "-  Fine-tune with the mini of balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785c976-110c-43fd-97e9-cd14bd02d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.applications.mobilenet_v3 import MobileNetV3Large, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.metrics import TopKCategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285bad3-5a67-4623-8c8e-ae273fc2ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = 224, 224 # 299 for inception-resnet # 224 for mobielnet\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0cf70-0e93-496d-bf54-eb8b27e97ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_TRAIN_DIR = 'data/train_mini_supercategory/Animalia/'\n",
    "NUM_CLASSES = len(os.listdir(IMG_TRAIN_DIR))\n",
    "print(f'There are {NUM_CLASSES} classes in {IMG_TRAIN_DIR}')\n",
    "\n",
    "MODEL_SAVEDIR = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb1075-528d-40a8-863c-870b88b4c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train datagen; no validation datagen here\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input, # preprocess input already scales to [-1,1]\n",
    "    validation_split=0.05,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    ")\n",
    "\n",
    "# attach the datagen; we will try to build a model with birds first\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = IMG_TRAIN_DIR,\n",
    "    target_size = (IMG_WIDTH, IMG_HEIGHT),\n",
    "    color_mode='rgb',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode = 'categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    directory = IMG_TRAIN_DIR,\n",
    "    target_size = (IMG_WIDTH, IMG_HEIGHT),\n",
    "    color_mode='rgb',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode = 'categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "assert '.ipynb_checkpoints' not in val_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1ecfe-e5a7-4a66-ba66-a794136d8f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import basemodel Inception ResnetV2 with imagenet weights\n",
    "base_model=MobileNetV3Large(\n",
    "    weights='imagenet',\n",
    "    include_top=False, \n",
    "    input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
    ")\n",
    "print(f\"Number of layers: {len(base_model.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4f1c63-d297-4efb-b129-32047a313deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27237ef-e77d-431a-88f1-11f59e2f4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach on top layer\n",
    "x = base_model.output\n",
    "# flatten and dense\n",
    "x = Flatten()(x)\n",
    "output = Dense(NUM_CLASSES, activation='softmax')(x) # number of classes\n",
    "model = Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df25727-4f97-434d-81d2-ad23d069de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all base layers at first\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f8797-3338-4b8c-89d4-46e7472e4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decay every 2 epochs (2* each step; a step is where each gradient update happens)\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.045, decay_steps=(2*train_generator.samples//BATCH_SIZE) , decay_rate=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb5a50-6117-4562-9df3-60cfea3689a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "top_k_metric = TopKCategoricalAccuracy(k=5)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=RMSprop(momentum=0.9, learning_rate=lr_schedule), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy', top_k_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf39be-61c0-41cd-b611-50a98d5bc6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Saving models in {MODEL_SAVEDIR}')\n",
    "checkpoint = ModelCheckpoint(filepath=MODEL_SAVEDIR, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ab171-8e71-4e37-843f-090e2afb9702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//BATCH_SIZE,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples//BATCH_SIZE,\n",
    "    epochs = 20,\n",
    "    callbacks=callbacks_list\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde29c2-993a-4031-a4d2-3452a688d41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# return evaluation metrics\n",
    "score = model.evaluate(val_generator)\n",
    "\n",
    "print (\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n",
    "print (\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1dcd0a-5d8f-4f84-af22-55cd4330d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f30cc-a711-4240-adcb-6414f5f5c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze and train on all layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af1a22-dd8e-40b6-aeb7-911f39808607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsure about adjusting the learning rate here..; comment out for now and see how accuracy fares\n",
    "# decay every 2 epochs (2* each step; a step is where each gradient update happens)\n",
    "# lr_schedule = ExponentialDecay(initial_learning_rate=0.0045, decay_steps=(4*train_generator.samples//BATCH_SIZE) , decay_rate=0.94)\n",
    "# model.compile(\n",
    "#     optimizer=RMSprop(momentum=0.9, learning_rate=lr_schedule), \n",
    "#     loss='categorical_crossentropy', \n",
    "#     metrics=['accuracy', top_k_metric]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773f2e3-af6c-4a55-a1e9-cacf1801ae41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-tune\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//BATCH_SIZE,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples//BATCH_SIZE,\n",
    "    epochs = 100,\n",
    "    callbacks=callbacks_list\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c3b73-a9c0-4949-9ba2-cf983e01a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return evaluation metrics\n",
    "score = model.evaluate(val_generator)\n",
    "print(score)\n",
    "\n",
    "print (\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n",
    "print (\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eaedd0-eabd-44f0-bcfe-a3f93624b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294918f-eef7-485d-893c-8b2583911897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3799d-2d07-46fd-bfc4-5c710853fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "# load image and test\n",
    "test_image = image.load_img('test_starfish.jpg', target_size = (IMG_WIDTH, IMG_HEIGHT))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = preprocess_input(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image) # logits layer of our model\n",
    "    \n",
    "label_map = val_generator.class_indices\n",
    "reverse_label_map = {i:k for k,i in label_map.items()} # indices to value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d734d-0138-41b6-8ae3-3339dcdff105",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(test_image, batch_size=1)[0]\n",
    "top_5_args = np.argsort(result)[::-1][:5]\n",
    "print('Top 5 most likely species:')\n",
    "for arg in top_5_args:\n",
    "    print(reverse_label_map[arg], f\"{100*result[arg]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ea29c-c1fe-4816-acda-067487c7791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7131e57-bbf0-4730-a149-bba5e3ad574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all nan at the moment\n",
    "# https://keras.io/examples/vision/grad_cam/\n",
    "heatmap = make_gradcam_heatmap(test_image, model, 'Conv_1')\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa09cc9-7e7b-4d20-918c-bf3162bb9f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Load the original image\n",
    "    img = keras.preprocessing.image.load_img(img_path)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))\n",
    "\n",
    "\n",
    "save_and_display_gradcam('test_starfish.jpg', heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cfe22-7cb4-44dc-ae6d-acbd2cb0de05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
